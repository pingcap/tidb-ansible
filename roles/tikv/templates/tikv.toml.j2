{% set my_ip = ansible_default_ipv4.address -%}
{# my id in tikv cluster #}
{% set my_peer_id = groups.tikv_servers.index(inventory_hostname) + 1 -%}
{% set all_pd = [] -%}
{% set pd_hosts = groups.pd_servers %}
{% for host in pd_hosts -%}
  {% set pd_ip = hostvars[host].ansible_default_ipv4.address -%}
  {% set pd_port = hostvars[host].pd_client_port -%}
  {% set _ = all_pd.append("%s:%s" % (pd_ip, pd_port)) -%}
{% endfor -%}

{% set metric_host = hostvars[groups.monitoring_servers[0]].ansible_default_ipv4.address if groups.monitoring_servers else '' -%}
{% set metric_port = hostvars[groups.monitoring_servers[0]].pushgateway_port if groups.monitoring_servers else ''-%}
{% set metric_interval = "0s" if metric_host == '' else tikv_metric_interval %}

# TiKV config template
#  Human-readable big numbers:
#   File size(based on byte): KB, MB, GB, TB, PB (or lowercase)
#    e.g.: 1_048_576 = "1MB"
#   Time(based on ms): ms, s, m, h
#    e.g.: 78_000 = "1.3m"

[server]
# listening address.
addr = "{{ my_ip }}:{{ tikv_port }}"
# leave empty, fallback to use addr
advertise-addr = ""

# set which dsn to use, warning: default is rocksdb without persistent.
dsn = "rocksdb"
# set the path to rocksdb directory.
store = "{{ tikv_data_dir }}"

# log level: trace, debug, info, warn, error, off.
log-level = "{{ tikv_log_level }}"

log-file = "{{ tikv_log_dir }}/{{ tikv_log_filename }}"

# notify capacity, 40960 is suitable for about 7000 regions.
notify-capacity = 40960
# maximum number of messages can be processed in one tick.
messages-per-tick = 4096
# socket send/recv buffer size.
send-buffer-size = "128KB"
recv-buffer-size = "128KB"
# size of thread pool for endpoint task
end-point-concurrency = 14

# set store capacity, if no set, use unlimited or disk size later.
# 0 is unlimited.
# capacity = "{{ tikv_capacity }}"

[pd]
# pd endpoints
endpoints = "{{ all_pd | join(',') }}"

[metric]
# the Prometheus client push interval.
# Setting the value to 0s stops Prometheus client from pushing.
interval = "{{ metric_interval }}"

# the Prometheus pushgateway address. Leaving it empty stops Prometheus client from pushing.
address = "{{ metric_host ~ ":" ~ metric_port if metric_host else "" }}"

# the Prometheus client push job name. Note: A node id will automatically append,
# e.g., "tikv_1".
job = "{{ tikv_metric_job_name_prefix }}"

[raftstore]
# notify capacity, 40960 is suitable for about 7000 regions.
notify-capacity = 40960
# maximum number of messages can be processed in one tick.
messages-per-tick = 4096

# Region heartbeat tick interval (ms) for reporting to pd.
pd-heartbeat-tick-interval = "5000ms"
# Store heartbeat tick interval (ms) for reporting to pd.
pd-store-heartbeat-tick-interval = "10000ms"

# When the region's size exceeds region-max-size, we will split the region
# into two which the left region's size will be region-split-size or a little
# bit smaller.
region-max-size = "80MB"
region-split-size = "64MB"
# When region size changes exceeds region-split-check-diff, we should check
# whether the region should be split or not.
region-split-check-diff = "8MB"

# When a peer hasn't been active for max-peer-down-duration,
# we will consider this peer to be down and report it to pd.
max-peer-down-duration = "5m"

region-compact-delete-keys-count = 1000000

[rocksdb]
# compression method (if any) is used to compress a block.
#   no:     kNoCompression
#   snappy: kSnappyCompression
#   zlib:   kZlibCompression
#   bzip2:  kBZip2Compression
#   lz4:    kLZ4Compression
#   lz4hc:  kLZ4HCCompression

# per level compression
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"

# Amount of data to build up in memory (backed by an unsorted log
# on disk) before converting to a sorted on-disk file.
write-buffer-size = "64MB"

# The maximum number of write buffers that are built up in memory.
max-write-buffer-number = 5

# The minimum number of write buffers that will be merged together
# before writing to storage.
min-write-buffer-number-to-merge = 1

# Maximum number of concurrent background compaction jobs, submitted to
# the default LOW priority thread pool.
max-background-compactions = 4

# Control maximum total data size for base level (level 1).
max-bytes-for-level-base = "64MB"

# Target file size for compaction.
target-file-size-base = "16MB"

# If true, the database will be created if it is missing.
create-if-missing = true

# Soft limit on number of level-0 files. We start slowing down writes at this point.
level0-slowdown-writes-trigger = 12

# Maximum number of level-0 files.  We stop writes at this point.
level0-stop-writes-trigger = 16

#enable-statistics = true
#stats-dump-period-sec = 60

[rocksdb.writecf]
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"
write-buffer-size = "64MB"
max-write-buffer-number = 5
min-write-buffer-number-to-merge = 1
max-bytes-for-level-base = "64MB"
target-file-size-base = "16MB"
block-cache-size = "2GB"

[rocksdb.raftcf]
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"
write-buffer-size = "64MB"
max-write-buffer-number = 5
min-write-buffer-number-to-merge = 1
max-bytes-for-level-base = "128MB"
target-file-size-base = "16MB"
block-cache-size = "2GB"

# For detailed explanation please refer to https://github.com/facebook/rocksdb/blob/master/include/rocksdb/table.h
[rocksdb.block-based-table]
# Approximate size of user data packed per block.  Note that the
# block size specified here corresponds to uncompressed data.
block-size = "64KB"

# block-cache used to cache uncompressed blocks, big block-cache can speed up read
block-cache-size = "10GB"

# If you're doing point lookups you definitely want to turn bloom filters on, We use
# bloom filters to avoid unnecessary disk reads. Default bits_per_key is 10, which
# yields ~1% false positive rate. Larger bits_per_key values will reduce false positive
# rate, but increase memory usage and space amplification.
bloom-filter-bits-per-key = 10

# false means one sst file one bloom filter, true means evry block has a corresponding bloom filter
block-based-bloom-filter = false

[rocksdb.defaultcf]
block-size = "64KB"
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"
write-buffer-size = "64MB"
max-write-buffer-number = 5
min-write-buffer-number-to-merge = 1
max-bytes-for-level-base = "64MB"
target-file-size-base = "16MB"
block-cache-size = "10GB"

[storage]
# notify capacity of scheduler's channel
scheduler-notify-capacity = 10240

# maximum number of messages can be processed in one tick
scheduler-messages-per-tick = 1024

scheduler-concurrency = 10240

# scheduler's worker pool size
scheduler-worker-pool-size = 4
