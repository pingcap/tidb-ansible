---
## The default configuration file for TiKV in YAML format

## TiKV config template
##  Human-readable big numbers:
##   File size(based on byte): KB, MB, GB, TB, PB
##    e.g.: 1_048_576 = "1MB"
##   Time(based on ms): ms, s, m, h
##    e.g.: 78_000 = "1.3m"

global:
  ## Log levels: trace, debug, info, warning, error, critical.
  ## Note that `debug` and `trace` are only available in development builds.
  # log-level: "info"

  ## Timespan between rotating the log files.
  ## Once this timespan passes, log files will be rotated, i.e. existing log file will have a
  ## timestamp appended to its name and a new file will be created.
  # log-rotation-timespan: "24h"

readpool:
  ## Configurations for the single thread pool serving read requests.
  unified:
    ## The minimal working thread count of the thread pool.
    # min-thread-count: 1

    ## The maximum working thread count of the thread pool.
    ## The default value is max(4, LOGICAL_CPU_NUM * 0.8).
    # max-thread-count: 8

    ## Size of the stack for each thread in the thread pool.
    # stack-size: "10MB"

    ## Max running tasks of each worker, reject if exceeded.
    # max-tasks-per-worker: 2000

  storage:
    ## Whether to use the unified read pool to handle storage requests.
    # use-unified-pool: false

    ## The following configurations only take effect when `use-unified-pool` is false.

    ## Size of the thread pool for high-priority operations.
    # high-concurrency: 4

    ## Size of the thread pool for normal-priority operations.
    # normal-concurrency: 4

    ## Size of the thread pool for low-priority operations.
    # low-concurrency: 4

    ## Max running high-priority operations of each worker, reject if exceeded.
    # max-tasks-per-worker-high: 2000

    ## Max running normal-priority operations of each worker, reject if exceeded.
    # max-tasks-per-worker-normal: 2000

    ## Max running low-priority operations of each worker, reject if exceeded.
    # max-tasks-per-worker-low: 2000

    ## Size of the stack for each thread in the thread pool.
    # stack-size: "10MB"

  coprocessor:
    ## Whether to use the unified read pool to handle storage requests.
    # use-unified-pool: true

    ## The following configurations only take effect when `use-unified-pool` is false.

    ## Most read requests from TiDB are sent to the coprocessor of TiKV. high/normal/low-concurrency is
    ## used to set the number of threads of the coprocessor.
    ## If there are many read requests, you can increase these config values (but keep it within the
    ## number of system CPU cores). For example, for a 32-core machine deployed with TiKV, you can even
    ## set these config to 30 in heavy read scenarios.
    ## If CPU_NUM > 8, the default thread pool size for coprocessors is set to CPU_NUM * 0.8.

    # high-concurrency: 8
    # normal-concurrency: 8
    # low-concurrency: 8
    # max-tasks-per-worker-high: 2000
    # max-tasks-per-worker-normal: 2000
    # max-tasks-per-worker-low: 2000
    # stack-size: "10MB"

server:
  ## Advertise listening address for client communication.
  ## If not set, `addr` will be used.
  # advertise-addr: ""

  ## Size of the thread pool for the gRPC server.
  # grpc-concurrency: 4

  ## The number of max concurrent streams/requests on a client connection.
  # grpc-concurrent-stream: 1024

  ## The number of connections with each TiKV server to send Raft messages.
  # grpc-raft-conn-num: 1

  ## Amount to read ahead on individual gRPC streams.
  # grpc-stream-initial-window-size: "2MB"

  ## Time to wait before sending out a ping to check if server is still alive.
  ## This is only for communications between TiKV instances.
  # grpc-keepalive-time: "10s"

  ## Time to wait before closing the connection without receiving KeepAlive ping Ack.
  # grpc-keepalive-timeout: "3s"

  ## How many snapshots can be sent concurrently.
  # concurrent-send-snap-limit: 32

  ## How many snapshots can be received concurrently.
  # concurrent-recv-snap-limit: 32

  ## Max allowed recursion level when decoding Coprocessor DAG expression.
  # end-point-recursion-limit: 1000

  ## Max time to handle Coprocessor requests before timeout.
  # end-point-request-max-handle-duration: "60s"

  ## Max bytes that snapshot can be written to disk in one second.
  ## It should be set based on your disk performance.
  # snap-max-write-bytes-per-sec: "100MB"

  ## Attributes about this server, e.g. `{ zone = "us-west-1", disk = "ssd" }`.
  # labels: {}

storage:
  ## The number of slots in Scheduler latches, which controls write concurrency.
  ## In most cases you can use the default value. When importing data, you can set it to a larger
  ## value, but no more than 2097152
  # scheduler-concurrency: 524288

  ## Scheduler's worker pool size, i.e. the number of write threads.
  ## It should be less than total CPU cores. When there are frequent write operations, set it to a
  ## higher value. More specifically, you can run `top -H -p tikv-pid` to check whether the threads
  ## named `sched-worker-pool` are busy.
  # scheduler-worker-pool-size: 4

  ## When the pending write bytes exceeds this threshold, the "scheduler too busy" error is displayed.
  # scheduler-pending-write-threshold: "100MB"

  ## TiKV will create a temporary file in {{data-dir}} to reserve some space, which is named 'space_placeholder_file'.
  ## When the disk has no free space you could remove this temporary file so thath TiKV can execute compaction
  ## job to reclaim disk space, which requires some extra temporary space.
  # reserve-space: "2GB"

  block-cache:
    ## Whether to create a shared block cache for all RocksDB column families.
    ##
    ## Block cache is used by RocksDB to cache uncompressed blocks. Big block cache can speed up
    ## read. It is recommended to turn on shared block cache. Since only the total cache size need
    ## to be set, it is easier to config. In most cases it should be able to auto-balance cache
    ## usage between column families with standard LRU algorithm.
    ##
    ## The rest of config in the storage.block-cache session is effective only when shared block
    ## cache is on.
    # shared: true

    ## Size of the shared block cache. Normally it should be tuned to 30%-50% of system's total
    ## memory. When the config is not set, it is decided by the sum of the following fields or
    ## their default value:
    ##   * rocksdb.defaultcf.block-cache-size or 25% of system's total memory
    ##   * rocksdb.writecf.block-cache-size   or 15% of system's total memory
    ##   * rocksdb.lockcf.block-cache-size    or  2% of system's total memory
    ##   * raftdb.defaultcf.block-cache-size  or  2% of system's total memory
    ##
    ## To deploy multiple TiKV nodes on a single physical machine, configure this parameter
    ## explicitly. Otherwise, the OOM problem might occur in TiKV.
    # capacity: "1GB"

pd:
  ## PD endpoints.
  # endpoints: []

metric:
  ## Prometheus client push interval.
  ## Setting the value to 0s stops Prometheus client from pushing.
  # interval: "15s"

  ## Prometheus PushGateway address.
  ## Leaving it empty stops Prometheus client from pushing.
  # address: ""

  ## Prometheus client push job name.
  ## Note: A node id will automatically append, e.g., "tikv_1".
  # job: "tikv"

raftstore:
  ## Store capacity, i.e. max data size allowed.
  ## If it is not set, disk capacity is used.
  # capacity: 0

  ## Internal notify capacity.
  ## 40960 is suitable for about 7000 Regions. It is recommended to use the default value.
  # notify-capacity: 40960

  ## Maximum number of internal messages to process in a tick.
  # messages-per-tick: 4096

  ## Region heartbeat tick interval for reporting to PD.
  # pd-heartbeat-tick-interval: "60s"

  ## Store heartbeat tick interval for reporting to PD.
  # pd-store-heartbeat-tick-interval: "10s"

  ## How long the peer will be considered down and reported to PD when it hasn't been active for this
  ## time.
  # max-peer-down-duration: "5m"

  ## Interval to check whether to start manual compaction for a Region.
  # region-compact-check-interval: "5m"

  ## Interval (s) to check Region whether the data are consistent.
  # consistency-check-interval: 0

  ## Delay time before deleting a stale peer.
  # clean-stale-peer-delay: "10m"

  ## Use how many threads to handle log apply
  # apply-pool-size: 2

  ## Use how many threads to handle raft messages
  # store-pool-size: 2

coprocessor:

rocksdb:
  ## Maximum number of threads of RocksDB background jobs.
  ## The background tasks include compaction and flush. For detailed information why RocksDB needs to
  ## do compaction, see RocksDB-related materials. When write traffic (like the importing data size)
  ## is big, it is recommended to enable more threads. But set the number of the enabled threads
  ## smaller than that of CPU cores. For example, when importing data, for a machine with a 32-core
  ## CPU, set the value to 28.
  # max-background-jobs: 8

  ## Represents the maximum number of threads that will concurrently perform a sub-compaction job by
  ## breaking it into multiple, smaller ones running simultaneously.
  # max-sub-compactions: 1

  ## Number of open files that can be used by the DB.
  ## Value -1 means files opened are always kept open and RocksDB will prefetch index and filter
  ## blocks into block cache at startup. So if your database has a large working set, it will take
  ## several minutes to open the DB. You may need to increase this if your database has a large
  ## working set. You can estimate the number of files based on `target-file-size-base` and
  ## `target_file_size_multiplier` for level-based compaction.
  # max-open-files: 40960

  ## RocksDB Write-Ahead Logs (WAL) recovery mode.
  ## 0 : TolerateCorruptedTailRecords, tolerate incomplete record in trailing data on all logs;
  ## 1 : AbsoluteConsistency, We don't expect to find any corruption in the WAL;
  ## 2 : PointInTimeRecovery, Recover to point-in-time consistency;
  ## 3 : SkipAnyCorruptedRecords, Recovery after a disaster;
  # wal-recovery-mode: 2

  ## RocksDB WAL directory.
  ## This config specifies the absolute directory path for WAL.
  ## If it is not set, the log files will be in the same directory as data. When you set the path to
  ## RocksDB directory in memory like in `/dev/shm`, you may want to set`wal-dir` to a directory on a
  ## persistent storage. See https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database .
  ## If there are two disks on the machine, storing RocksDB data and WAL logs on different disks can
  ## improve performance.
  # wal-dir: "/tmp/tikv/store"

  ## The following two fields affect how archived WAL will be deleted.
  ## 1. If both values are set to 0, logs will be deleted ASAP and will not get into the archive.
  ## 2. If `wal-ttl-seconds` is 0 and `wal-size-limit` is not 0, WAL files will be checked every 10
  ##    min and if total size is greater than `wal-size-limit`, they will be deleted starting with the
  ##    earliest until `wal-size-limit` is met. All empty files will be deleted.
  ## 3. If `wal-ttl-seconds` is not 0 and `wal-size-limit` is 0, then WAL files will be checked every
  ##    `wal-ttl-seconds / 2` and those that are older than `wal-ttl-seconds` will be deleted.
  ## 4. If both are not 0, WAL files will be checked every 10 min and both checks will be performed
  ##    with ttl being first.
  ## When you set the path to RocksDB directory in memory like in `/dev/shm`, you may want to set
  ## `wal-ttl-seconds` to a value greater than 0 (like 86400) and backup your DB on a regular basis.
  ## See https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database .
  # wal-ttl-seconds: 0
  # wal-size-limit: 0

  ## Max RocksDB WAL size in total
  # max-total-wal-size: "4GB"

  ## RocksDB Statistics provides cumulative stats over time.
  ## Turning statistics on will introduce about 5%-10% overhead for RocksDB, but it can help you to
  ## know the internal status of RocksDB.
  # enable-statistics: true

  ## Dump statistics periodically in information logs.
  ## Same as RocksDB's default value (10 min).
  # stats-dump-period: "10m"

  ## Refer to: https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ
  ## If you want to use RocksDB on multi disks or spinning disks, you should set value at least 2MB.
  # compaction-readahead-size: 0

  ## Max buffer size that is used by WritableFileWrite.
  # writable-file-max-buffer-size: "1MB"

  ## Use O_DIRECT for both reads and writes in background flush and compactions.
  # use-direct-io-for-flush-and-compaction: false

  ## Allows OS to incrementally sync files to disk while they are being written, asynchronously,
  ## in the background.
  # bytes-per-sync: "1MB"

  ## Allows OS to incrementally sync WAL to disk while it is being written.
  # wal-bytes-per-sync: "512KB"

  ## Options for `Titan`.
  titan:
  ## Enables or disables `Titan`. Note that Titan is still an experimental feature. Once
  ## enabled, it can't fall back. Forced fallback may result in data loss.
  # enabled: false

  ## Maximum number of threads of `Titan` background gc jobs.
  # max-background-gc: 1

  ## Options for "Default" Column Family, which stores actual user data.
  defaultcf:
    ## Compression method (if any) is used to compress a block.
    ##   no:     kNoCompression
    ##   snappy: kSnappyCompression
    ##   zlib:   kZlibCompression
    ##   bzip2:  kBZip2Compression
    ##   lz4:    kLZ4Compression
    ##   lz4hc:  kLZ4HCCompression
    ##   zstd:   kZSTD
    ## `lz4` is a compression algorithm with moderate speed and compression ratio. The compression
    ## ratio of `zlib` is high. It is friendly to the storage space, but its compression speed is
    ## slow. This compression occupies many CPU resources.

    ## Per level compression.
    ## This config should be chosen carefully according to CPU and I/O resources. For example, if you
    ## use the compression mode of "no:no:lz4:lz4:lz4:zstd:zstd" and find much I/O pressure of the
    ## system (run the `iostat` command to find %util lasts 100%, or run the `top` command to find many
    ## iowaits) when writing (importing) a lot of data while the CPU resources are adequate, you can
    ## compress level-0 and level-1 and exchange CPU resources for I/O resources. If you use the
    ## compression mode of "no:no:lz4:lz4:lz4:zstd:zstd" and you find the I/O pressure of the system is
    ## not big when writing a lot of data, but CPU resources are inadequate. Then run the `top` command
    ## and choose the `-H` option. If you find a lot of bg threads (namely the compression thread of
    ## RocksDB) are running, you can exchange I/O resources for CPU resources and change the compression
    ## mode to "no:no:no:lz4:lz4:zstd:zstd". In a word, it aims at making full use of the existing
    ## resources of the system and improving TiKV performance in terms of the current resources.
    # compression-per-level: ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]

    ## The data block size. RocksDB compresses data based on the unit of block.
    ## Similar to page in other databases, block is the smallest unit cached in block-cache. Note that
    ## the block size specified here corresponds to uncompressed data.
    # block-size: "64KB"

    ## If you're doing point lookups you definitely want to turn bloom filters on. We use bloom filters
    ## to avoid unnecessary disk reads. Default bits_per_key is 10, which yields ~1% false positive
    ## rate. Larger `bloom-filter-bits-per-key` values will reduce false positive rate, but increase
    ## memory usage and space amplification.
    # bloom-filter-bits-per-key: 10

    # level0-file-num-compaction-trigger: 4

    ## Soft limit on number of level-0 files.
    ## When the number of SST files of level-0 reaches the limit of `level0-slowdown-writes-trigger`,
    ## RocksDB tries to slow down the write operation, because too many SST files of level-0 can cause
    ## higher read pressure of RocksDB.
    # level0-slowdown-writes-trigger: 20

    ## Maximum number of level-0 files.
    ## When the number of SST files of level-0 reaches the limit of `level0-stop-writes-trigger`,
    ## RocksDB stalls the new write operation.
    # level0-stop-writes-trigger: 36

    ## Amount of data to build up in memory (backed by an unsorted log on disk) before converting to a
    ## sorted on-disk file. It is the RocksDB MemTable size.
    # write-buffer-size: "128MB"

    ## The maximum number of the MemTables. The data written into RocksDB is first recorded in the WAL
    ## log, and then inserted into MemTables. When the MemTable reaches the size limit of
    ## `write-buffer-size`, it turns into read only and generates a new MemTable receiving new write
    ## operations. The flush threads of RocksDB will flush the read only MemTable to the disks to become
    ## an SST file of level0. `max-background-flushes` controls the maximum number of flush threads.
    ## When the flush threads are busy, resulting in the number of the MemTables waiting to be flushed
    ## to the disks reaching the limit of `max-write-buffer-number`, RocksDB stalls the new operation.
    ## "Stall" is a flow control mechanism of RocksDB. When importing data, you can set the
    ## `max-write-buffer-number` value higher, like 10.
    # max-write-buffer-number: 5

    ## The minimum number of write buffers that will be merged together before writing to storage.
    # min-write-buffer-number-to-merge: 1

    ## Control maximum total data size for base level (level 1).
    ## When the level-1 data size reaches the limit value of `max-bytes-for-level-base`, the SST files
    ## of level-1 and their overlap SST files of level-2 will be compacted. The golden rule: the first
    ## reference principle of setting `max-bytes-for-level-base` is guaranteeing that the
    ## `max-bytes-for-level-base` value is roughly equal to the data volume of level-0. Thus
    ## unnecessary compaction is reduced. For example, if the compression mode is
    ## "no:no:lz4:lz4:lz4:lz4:lz4", the `max-bytes-for-level-base` value can be `write-buffer-size * 4`,
    ## because there is no compression of level-0 and level-1 and the trigger condition of compaction
    ## for level-0 is that the number of the SST files reaches 4 (the default value). When both level-0
    ## and level-1 adopt compaction, it is necessary to analyze RocksDB logs to know the size of an SST
    ## file compressed from a MemTable. For example, if the file size is 32MB, the proposed value of
    ## `max-bytes-for-level-base` is 32MB * 4 = 128MB.
    # max-bytes-for-level-base: "512MB"

    ## Target file size for compaction.
    ## The SST file size of level-0 is influenced by the compaction algorithm of `write-buffer-size`
    ## and level0. `target-file-size-base` is used to control the size of a single SST file of level1 to
    ## level6.
    # target-file-size-base: "8MB"

    ## Max bytes for `compaction.max_compaction_bytes`.
    # max-compaction-bytes: "2GB"

    ## There are four different compaction priorities.
    ## 0 : ByCompensatedSize
    ## 1 : OldestLargestSeqFirst
    ## 2 : OldestSmallestSeqFirst
    ## 3 : MinOverlappingRatio
    # compaction-pri: 3

    ## Enable read amplification statistics.
    ## value  =>  memory usage (percentage of loaded blocks memory)
    ## 1      =>  12.50 %
    ## 2      =>  06.25 %
    ## 4      =>  03.12 %
    ## 8      =>  01.56 %
    ## 16     =>  00.78 %
    # read-amp-bytes-per-bit: 0

    ## Options for "Titan" for "Default" Column Family
    titan:
      ## The smallest value to store in blob files. Value smaller than
      ## this threshold will be inlined in base DB.
      # min-blob-size: "1KB"

      ## The compression algorithm used to compress data in blob files.
      ## Compression method.
      ##   no:     kNoCompression
      ##   snappy: kSnappyCompression
      ##   zlib:   kZlibCompression
      ##   bzip2:  kBZip2Compression
      ##   lz4:    kLZ4Compression
      ##   lz4hc:  kLZ4HCCompression
      ##   zstd:   kZSTD
      # blob-file-compression: "lz4"

      ## Specifics cache size for blob records
      # blob-cache-size: "0GB"

      ## If the ratio of discardable size of a blob file is larger than
      ## this threshold, the blob file will be GCed out.
      # discardable-ratio: 0.5

      ## The mode used to process blob files. In read-only mode Titan
      ## stops writing value into blob log. In fallback mode Titan
      ## converts blob index into real value on flush and compaction.
      ## This option is especially useful for downgrading Titan.
      ##   default:   kNormal
      ##   read-only: kReadOnly
      ##   fallback:  kFallback
      # blob-run-mode: "normal"

  ## Options for "Write" Column Family, which stores MVCC commit information
  writecf:
    ## Recommend to set it the same as `rocksdb.defaultcf.compression-per-level`.
    # compression-per-level: ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]
    # block-size: "64KB"

    ## Recommend to set it the same as `rocksdb.defaultcf.write-buffer-size`.
    # write-buffer-size: "128MB"
    # max-write-buffer-number: 5
    # min-write-buffer-number-to-merge: 1

    ## Recommend to set it the same as `rocksdb.defaultcf.max-bytes-for-level-base`.
    # max-bytes-for-level-base: "512MB"
    # target-file-size-base: "8MB"

    # level0-file-num-compaction-trigger: 4
    # level0-slowdown-writes-trigger: 20
    # level0-stop-writes-trigger: 36
    # cache-index-and-filter-blocks: true
    # pin-l0-filter-and-index-blocks: true
    # compaction-pri: 3
    # read-amp-bytes-per-bit: 0
    # dynamic-level-bytes: true

  lockcf:
    # compression-per-level: ["no", "no", "no", "no", "no", "no", "no"]
    # block-size: "16KB"
    # write-buffer-size: "128MB"
    # max-write-buffer-number: 5
    # min-write-buffer-number-to-merge: 1
    # max-bytes-for-level-base: "128MB"
    # target-file-size-base: "8MB"
    # level0-slowdown-writes-trigger: 20
    # level0-stop-writes-trigger: 36
    # cache-index-and-filter-blocks: true
    # pin-l0-filter-and-index-blocks: true
    # compaction-pri: 0
    # read-amp-bytes-per-bit: 0
    # dynamic-level-bytes: true

raftdb:
  # max-background-jobs: 4
  # max-sub-compactions: 2
  # max-open-files: 40960
  # max-manifest-file-size: "20MB"
  # create-if-missing: true

  # enable-statistics: true
  # stats-dump-period: "10m"

  # compaction-readahead-size: 0
  # writable-file-max-buffer-size: "1MB"
  # use-direct-io-for-flush-and-compaction: false
  # enable-pipelined-write: true
  # allow-concurrent-memtable-write: false
  # bytes-per-sync: "1MB"
  # wal-bytes-per-sync: "512KB"

  # info-log-max-size: "1GB"
  # info-log-roll-time: "0"
  # info-log-keep-log-file-num: 10
  # info-log-dir: ""
  # optimize-filters-for-hits: true

  defaultcf:
    ## Recommend to set it the same as `rocksdb.defaultcf.compression-per-level`.
    # compression-per-level: ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]
    # block-size: "64KB"

    ## Recommend to set it the same as `rocksdb.defaultcf.write-buffer-size`.
    # write-buffer-size: "128MB"
    # max-write-buffer-number: 5
    # min-write-buffer-number-to-merge: 1

    ## Recommend to set it the same as `rocksdb.defaultcf.max-bytes-for-level-base`.
    # max-bytes-for-level-base: "512MB"
    # target-file-size-base: "8MB"

    # level0-file-num-compaction-trigger: 4
    # level0-slowdown-writes-trigger: 20
    # level0-stop-writes-trigger: 36
    # cache-index-and-filter-blocks: true
    # pin-l0-filter-and-index-blocks: true
    # compaction-pri: 0
    # read-amp-bytes-per-bit: 0
    # dynamic-level-bytes: true
    # optimize-filters-for-hits: true

security:
  ## The path for TLS certificates. Empty string means disabling secure connections.
  # ca-path: ""
  # cert-path: ""
  # key-path: ""
  # cert-allowed-cn: []

  ## Configurations for encryption at rest. Experimental.
  encryption:
    ## Encryption method to use for data files.
    ## Possible values are "plaintext", "aes128-ctr", "aes192-ctr" and "aes256-ctr". Value other than
    ## "plaintext" means encryption is enabled, in which case master key must be specified.
    # data-encryption-method: "plaintext"
    
    ## Specifies how often TiKV rotates data encryption key.
    # data-key-rotation-period = "7d"
    
    ## Specifies master key if encryption is enabled. There are three types of master key:
    ##
    ##   * "plaintext":
    ##
    ##     Plaintext as master key means no master key is given and only applicable when
    ##     encryption is not enabled, i.e. data-encryption-method = "plaintext". This type doesn't
    ##     have sub-config items. Example:
    ##     
    ##     master-key:
    ##       type: "plaintext"
    ##
    ##   * "kms":
    ##
    ##     Use a KMS service to supply master key. Currently only AWS KMS is supported. This type of
    ##     master key is recommended for production use. Example:
    ##
    ##     master-key:
    ##       type: "kms"
    ##       ## KMS CMK key id. Must be a valid KMS CMK where the TiKV process has access to.
    ##       ## In production is recommended to grant access of the CMK to TiKV using IAM.
    ##       key-id = "1234abcd-12ab-34cd-56ef-1234567890ab"
    ##       ## AWS region of the KMS CMK.
    ##       region: "us-west-2"
    ##       ## (Optional) AWS KMS service endpoint. Only required when non-default KMS endpoint is
    ##       ## desired.
    ##       endpoint: "https://kms.us-west-2.amazonaws.com"
    ##
    ##   * "file":
    ##
    ##     Supply a custom encryption key stored in a file. It is recommended NOT to use in production,
    ##     as it breaks the purpose of encryption at rest, unless the file is stored in tempfs.
    ##     The file must contain a 256-bits (32 bytes, regardless of key length implied by 
    ##     data-encryption-method) key encoded as hex string and end with newline ("\n"). Example:
    ##
    ##     master-key:
    ##       type: "file"
    ##       path: "/path/to/master/key/file"
    ##
    # master-key:
    #   type = "plaintext"
    
    ## Specifies the old master key when rotating master key. Same config format as master-key.
    ## The key is only access once during TiKV startup, after that TiKV do not need access to the key.
    ## And it is okay to leave the stale previous-master-key config after master key rotation.
    # previous-master-key:
    #   type: "plaintext"

import:

pessimistic_txn:
  ## Enable pessimistic transaction
  # enabled: true

  ## Time to wait in milliseconds before responding to TiDB when pessimistic
  ## transactions encounter locks
  # wait-for-lock-timeout: "1s"

  ## If more than one transaction is waiting for the same lock, only the one with smallest
  ## start timestamp will be waked up immediately when the lock is released. Others will
  ## be waked up after `wake_up_delay_duration(ms)` to reduce contention and make the oldest
  ## one more likely acquires the lock.
  # wake-up-delay-duration: "20ms"

gc:
  ## The number of keys to GC in one batch.
  # batch-keys: 512

  ## Max bytes that GC worker can write to rocksdb in one second.
  ## If it is set to 0, there is no limit.
  # max-write-bytes-per-sec: "0"
